{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938e8a2c",
   "metadata": {},
   "source": [
    "# Offline Behaviour Fetcher\n",
    "Reuse the load already-saved VirusTotal responses from `output/raw_responses` instead of calling the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8616c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37300d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f328b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports ready (offline cache mode)\n"
     ]
    }
   ],
   "source": [
    "# Core / stdlib\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime, timezone\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party libs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✅ Imports ready (offline cache mode)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac8ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config ready (reading cached responses)\n"
     ]
    }
   ],
   "source": [
    "# ---- OFFLINE CONFIG ----\n",
    "\n",
    "OUTPUT_DIR = \"./output\"\n",
    "RAW_DIR = os.path.join(OUTPUT_DIR, \"raw_responses\")\n",
    "INPUT_CSV = \"./output/hash_signature_output.csv\"\n",
    "\n",
    "TARGET_MIN_SAMPLES = 78    # minimum total samples to collect\n",
    "TARGET_MAX_SAMPLES = 100   # cap to avoid over-collection\n",
    "MIN_FAMILIES = 8           # at least 8 malware families\n",
    "\n",
    "HARD_REQUIRE_MINIMUMS = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "if not os.path.isdir(RAW_DIR):\n",
    "    raise SystemExit(f\"Expected cached responses in {RAW_DIR}.\")\n",
    "\n",
    "print(\"✅ Config ready (reading cached responses)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33c758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV helpers ready\n"
     ]
    }
   ],
   "source": [
    "def read_hash_csv(csv_path: str, hash_col: str = \"hash\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read hashes + metadata from a CSV.\n",
    "    Required column: 'hash'\n",
    "    Optional columns: 'malware_family', 'source'\n",
    "    - Deduplicates by 'hash'\n",
    "    - Lowercases and trims hash strings\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, dtype=str).fillna(\"\")\n",
    "    if hash_col not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain a '{hash_col}' column. Found: {list(df.columns)}\")\n",
    "    df['hash'] = df['hash'].str.strip().str.lower()\n",
    "\n",
    "    if 'malware_family' not in df.columns:\n",
    "        df['malware_family'] = \"Unknown\"\n",
    "    if 'source' not in df.columns:\n",
    "        df['source'] = \"Unknown\"\n",
    "\n",
    "    df = df[df['hash'] != \"\"].drop_duplicates(subset=['hash'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def assert_dataset_meets_requirements(df: pd.DataFrame,\n",
    "                                      min_samples: int,\n",
    "                                      min_families: int) -> None:\n",
    "    \"\"\"\n",
    "    Hard stop (or warn) if the dataset does not meet min counts.\n",
    "    - Unique hashes ≥ min_samples\n",
    "    - Family count ≥ min_families\n",
    "    \"\"\"\n",
    "    num_hashes = df['hash'].nunique()\n",
    "    num_fams = df['malware_family'].replace(\"\", \"Unknown\").nunique()\n",
    "\n",
    "    problems = []\n",
    "    if num_hashes < min_samples:\n",
    "        problems.append(f\"- Only {num_hashes} unique hashes found (need ≥ {min_samples}).\")\n",
    "    if num_fams < min_families:\n",
    "        problems.append(f\"- Only {num_fams} malware families found (need ≥ {min_families}).\")\n",
    "\n",
    "    if problems and HARD_REQUIRE_MINIMUMS:\n",
    "        msg = (\n",
    "            \"\\nINPUT DATA DOES NOT MEET MINIMUMS:\\n\"\n",
    "            + \"\\n\".join(problems)\n",
    "        )\n",
    "        raise SystemExit(msg)\n",
    "    elif problems:\n",
    "        logging.warning(\"Proceeding despite dataset not meeting minimums:\\n\" + \"\\n\".join(problems))\n",
    "\n",
    "print(\"✅ CSV helpers ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12984b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sampler ready\n"
     ]
    }
   ],
   "source": [
    "def select_sample_hashes(df: pd.DataFrame,\n",
    "                         min_samples: int = TARGET_MIN_SAMPLES,\n",
    "                         max_samples: int = TARGET_MAX_SAMPLES,\n",
    "                         min_families: int = MIN_FAMILIES) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a balanced selection by iterating families in round-robin until we hit the target.\n",
    "    Ensures (best effort) ≥ min_families in the final selection.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['malware_family'] = df['malware_family'].replace(\"\", \"Unknown\")\n",
    "\n",
    "    # Enforce minimums on the whole dataset\n",
    "    assert_dataset_meets_requirements(df, min_samples=min_samples, min_families=min_families)\n",
    "\n",
    "    # Families ordered by frequency (more populous first)\n",
    "    families_ordered = df['malware_family'].value_counts().index.tolist()\n",
    "    logging.info(f\"Available families in input: {len(families_ordered)}\")\n",
    "\n",
    "    # Target amount (prefer min_samples, never exceed max_samples or dataset size)\n",
    "    target = min(max(min_samples, min(len(df), max_samples)), max_samples)\n",
    "    target = min(target, len(df))\n",
    "    logging.info(f\"Target sample count: {target}\")\n",
    "\n",
    "    # Family pools shuffled deterministically for reproducibility\n",
    "    pools: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for fam in families_ordered:\n",
    "        pools[fam] = df[df['malware_family'] == fam].sample(frac=1.0, random_state=42).to_dict(orient='records')\n",
    "\n",
    "    # Round-robin pick across families\n",
    "    selected: List[Dict[str, Any]] = []\n",
    "    families_rr = families_ordered[:]\n",
    "    fam_idx = 0\n",
    "    while len(selected) < target and families_rr:\n",
    "        fam = families_rr[fam_idx % len(families_rr)]\n",
    "        if pools[fam]:\n",
    "            selected.append(pools[fam].pop())\n",
    "            fam_idx += 1\n",
    "        else:\n",
    "            # remove empty families from rotation\n",
    "            families_rr.pop(fam_idx % len(families_rr))\n",
    "\n",
    "    sampled_df = pd.DataFrame(selected)\n",
    "\n",
    "    # Ensure ≥ min_families in the final selection (swap in if needed)\n",
    "    sel_fams = sampled_df['malware_family'].nunique()\n",
    "    if sel_fams < min_families:\n",
    "        logging.warning(f\"Selection has only {sel_fams} families; trying to add more...\")\n",
    "        remaining_rows = df[~df['hash'].isin(sampled_df['hash'])]\n",
    "        extras = (remaining_rows.groupby('malware_family').head(1).reset_index(drop=True))\n",
    "        for _, row in extras.iterrows():\n",
    "            if row['malware_family'] not in sampled_df['malware_family'].unique():\n",
    "                if len(sampled_df) < target:\n",
    "                    sampled_df = pd.concat([sampled_df, pd.DataFrame([row])], ignore_index=True)\n",
    "                else:\n",
    "                    # Replace one row from most common family to keep total unchanged\n",
    "                    over_fam = sampled_df['malware_family'].value_counts().index[0]\n",
    "                    idx_drop = sampled_df[sampled_df['malware_family'] == over_fam].index[0]\n",
    "                    sampled_df = sampled_df.drop(index=idx_drop)\n",
    "                    sampled_df = pd.concat([sampled_df, pd.DataFrame([row])], ignore_index=True)\n",
    "            if sampled_df['malware_family'].nunique() >= min_families:\n",
    "                break\n",
    "\n",
    "    logging.info(f\"Selected {len(sampled_df)} samples from {sampled_df['malware_family'].nunique()} families\")\n",
    "    return sampled_df.reset_index(drop=True)\n",
    "\n",
    "print(\"✅ Sampler ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f51f1268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cached response helpers ready\n"
     ]
    }
   ],
   "source": [
    "def load_cached_response(hash_val: str, suffix: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load <hash>_<suffix>.json from RAW_DIR and wrap it like the API helper did.\"\"\"\n",
    "    path = os.path.join(RAW_DIR, f\"{hash_val}_{suffix}.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return {'success': False, 'status_code': None, 'json': None, 'error': f\"missing cached {suffix} response\", 'path': path}\n",
    "    try:\n",
    "        with open(path, 'r') as fh:\n",
    "            data = json.load(fh)\n",
    "    except Exception as exc:\n",
    "        return {'success': False, 'status_code': None, 'json': None, 'error': f\"failed to read cached {suffix} response: {exc}\", 'path': path}\n",
    "\n",
    "    if isinstance(data, dict) and 'success' in data:\n",
    "        data.setdefault('path', path)\n",
    "        return data\n",
    "\n",
    "    return {'success': True, 'status_code': None, 'json': data, 'path': path}\n",
    "\n",
    "\n",
    "def get_file_report(hash_val: str) -> Dict[str, Any]:\n",
    "    \"\"\"Offline file metadata (previously saved VT file JSON).\"\"\"\n",
    "    return load_cached_response(hash_val, \"file\")\n",
    "\n",
    "\n",
    "def get_behavior_report(hash_val: str) -> Dict[str, Any]:\n",
    "    \"\"\"Offline behaviour summary (previously saved VT behaviour JSON).\"\"\"\n",
    "    return load_cached_response(hash_val, \"behavior\")\n",
    "\n",
    "print(\"✅ Cached response helpers ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d460aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Behaviour parser ready\n"
     ]
    }
   ],
   "source": [
    "def extract_behavioral_indicators(behav_resp: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse /behaviour_summary response safely into flattened indicators.\n",
    "    Returns empty lists when fields are missing (some hashes have no behaviour).\n",
    "    \"\"\"\n",
    "    indicators = {\n",
    "        'processes_created': [],\n",
    "        'files_written': [],\n",
    "        'files_deleted': [],\n",
    "        'registry_keys_set': [],\n",
    "        'registry_keys_deleted': [],\n",
    "        'dns_lookups': [],\n",
    "        'ip_traffic': [],\n",
    "        'http_conversations': [],\n",
    "        'command_executions': [],\n",
    "        'mutexes_created': [],\n",
    "        'services_created': [],\n",
    "        'tags': [],\n",
    "        'mitre_techniques': []\n",
    "    }\n",
    "\n",
    "    # If cached call missing or invalid JSON, return empty indicators\n",
    "    if not behav_resp or not behav_resp.get('success'):\n",
    "        return indicators\n",
    "\n",
    "    try:\n",
    "        j = behav_resp.get('json') or {}\n",
    "        if not isinstance(j, dict):\n",
    "            return indicators\n",
    "\n",
    "        # Expected nesting: json['data']\n",
    "        attrs = j.get('data') or {}\n",
    "\n",
    "        if not isinstance(attrs, dict):\n",
    "            return indicators\n",
    "\n",
    "        # Copy simple lists\n",
    "        indicators['processes_created'] = attrs.get('processes_created') or []\n",
    "        indicators['files_written']     = attrs.get('files_written')     or []\n",
    "        indicators['files_deleted']     = attrs.get('files_deleted')     or []\n",
    "        indicators['registry_keys_set'] = attrs.get('registry_keys_set') or []\n",
    "        indicators['registry_keys_deleted'] = attrs.get('registry_keys_deleted') or []\n",
    "        indicators['command_executions']    = attrs.get('command_executions')    or []\n",
    "        indicators['mutexes_created']       = attrs.get('mutexes_created')       or []\n",
    "        indicators['services_created']      = attrs.get('services_created')      or []\n",
    "\n",
    "        tags = attrs.get('tags') or []\n",
    "        if isinstance(tags, list):\n",
    "            indicators['tags'] = [str(tag).strip() for tag in tags if str(tag).strip()]\n",
    "\n",
    "        # Normalize structured items\n",
    "        dns_lookups = attrs.get('dns_lookups') or []\n",
    "        if isinstance(dns_lookups, list):\n",
    "            indicators['dns_lookups'] = [d.get('hostname', '') for d in dns_lookups if isinstance(d, dict)]\n",
    "\n",
    "        ip_traffic = attrs.get('ip_traffic') or []\n",
    "        if isinstance(ip_traffic, list):\n",
    "            indicators['ip_traffic'] = [\n",
    "                f\"{ip.get('destination_ip','')}:{ip.get('destination_port','')}\"\n",
    "                for ip in ip_traffic if isinstance(ip, dict)\n",
    "            ]\n",
    "\n",
    "        http_conversations = attrs.get('http_conversations') or []\n",
    "        if isinstance(http_conversations, list):\n",
    "            indicators['http_conversations'] = [h.get('url','') for h in http_conversations if isinstance(h, dict)]\n",
    "\n",
    "        mt = attrs.get('mitre_attack_techniques') or []\n",
    "        if isinstance(mt, list):\n",
    "            indicators['mitre_techniques'] = [t for t in mt if isinstance(t, dict)]\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing behavior response safely: {e}\")\n",
    "\n",
    "    return indicators\n",
    "\n",
    "print(\"✅ Behaviour parser ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88ae38d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ATT&CK layer builder ready\n"
     ]
    }
   ],
   "source": [
    "def build_attck_layer(tech_counter: Counter) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a minimal ATT&CK Navigator layer.\n",
    "    - technique.score = occurrence count across all samples\n",
    "    - tactic left blank (behaviour_summary may not map tactics)\n",
    "    \"\"\"\n",
    "    techniques = [\n",
    "        {\"techniqueID\": tid, \"tactic\": \"\", \"score\": int(count)}\n",
    "        for tid, count in tech_counter.items()\n",
    "    ]\n",
    "    return {\n",
    "        \"name\": \"VT → MITRE ATT&CK Layer\",\n",
    "        \"description\": \"Techniques extracted from cached VirusTotal behaviour summaries\",\n",
    "        \"domain\": \"enterprise-attack\",\n",
    "        \"version\": \"4.5\",\n",
    "        \"techniques\": techniques\n",
    "    }\n",
    "\n",
    "print(\"✅ ATT&CK layer builder ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5875c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collector ready (offline)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collect_samples(sample_df: pd.DataFrame,\n",
    "                    save_every: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each hash:\n",
    "      1) Load cached file report JSON from RAW_DIR\n",
    "      2) Load cached behaviour summary JSON from RAW_DIR\n",
    "      3) Parse indicators → row\n",
    "      4) Persist aggregated outputs (CSV/JSON + ATT&CK layer)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    tech_counter = Counter()\n",
    "\n",
    "    for idx, row in enumerate(tqdm(sample_df.to_dict(orient=\"records\"), desc=\"Parsing cached samples\"), start=1):\n",
    "        h = row['hash']\n",
    "        family = row.get('malware_family', 'Unknown')\n",
    "        source = row.get('source', 'Unknown')\n",
    "\n",
    "        # Initialize result row with defaults\n",
    "        result = {\n",
    "            'hash': h,\n",
    "            'family': family,\n",
    "            'source': source,\n",
    "            'status': 'failed',\n",
    "            'detection_ratio': None,\n",
    "            'first_seen': None,\n",
    "            'last_seen': None,\n",
    "            'processes_count': 0,\n",
    "            'files_written_count': 0,\n",
    "            'files_deleted_count': 0,\n",
    "            'registry_keys_set_count': 0,\n",
    "            'dns_lookups_count': 0,\n",
    "            'ip_connections_count': 0,\n",
    "            'http_requests_count': 0,\n",
    "            'mutexes_count': 0,\n",
    "            'mitre_techniques_count': 0,\n",
    "            'mitre_techniques': '',\n",
    "            'tags_count': 0,\n",
    "            'tags': '',\n",
    "            'collected_date': datetime.now(timezone.utc).isoformat(),\n",
    "        }\n",
    "\n",
    "        # ---- 1) File report (cached)\n",
    "        fr = get_file_report(h)\n",
    "        if not fr.get('success'):\n",
    "            result['error'] = fr.get('error', 'file_report_missing')\n",
    "            logging.warning(f\"Skipping {h}: {result['error']}\")\n",
    "            results.append(result)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            attrs = fr.get('json', {}).get('data', {}).get('attributes', {}) if isinstance(fr.get('json'), dict) else {}\n",
    "            stats = attrs.get('last_analysis_stats', {}) if isinstance(attrs, dict) else {}\n",
    "            total = sum(stats.values()) if isinstance(stats, dict) else 0\n",
    "            malicious = stats.get('malicious', 0) if isinstance(stats, dict) else 0\n",
    "            result['detection_ratio'] = f\"{malicious}/{total}\" if total else None\n",
    "            result['first_seen'] = attrs.get('first_submission_date') or None\n",
    "            result['last_seen'] = attrs.get('last_analysis_date') or None\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Error extracting stats for {h}: {e}\")\n",
    "\n",
    "        # ---- 2) Behaviour summary (cached)\n",
    "        br = get_behavior_report(h)\n",
    "        if not br.get('success'):\n",
    "            logging.warning(f\"No behaviour summary for {h}: {br.get('error', 'behavior_report_missing')}\")\n",
    "\n",
    "        indicators = extract_behavioral_indicators(br)\n",
    "\n",
    "        result.update({\n",
    "            'processes_count': len(indicators['processes_created']),\n",
    "            'files_written_count': len(indicators['files_written']),\n",
    "            'files_deleted_count': len(indicators['files_deleted']),\n",
    "            'registry_keys_set_count': len(indicators['registry_keys_set']),\n",
    "            'dns_lookups_count': len(indicators['dns_lookups']),\n",
    "            'ip_connections_count': len(indicators['ip_traffic']),\n",
    "            'http_requests_count': len(indicators['http_conversations']),\n",
    "            'mutexes_count': len(indicators['mutexes_created']),\n",
    "            'mitre_techniques_count': len(indicators['mitre_techniques']),\n",
    "            'mitre_techniques': \", \".join([t.get('id','') for t in indicators['mitre_techniques'] if isinstance(t, dict)]),\n",
    "            'tags_count': len(indicators['tags']),\n",
    "            'tags': \", \".join(indicators['tags']),\n",
    "        })\n",
    "\n",
    "        for t in indicators['mitre_techniques']:\n",
    "            if isinstance(t, dict) and t.get('id'):\n",
    "                tech_counter[t['id']] += 1\n",
    "\n",
    "        # Mark row success and append\n",
    "        result['status'] = 'success'\n",
    "        results.append(result)\n",
    "\n",
    "        # Periodic save of partial progress\n",
    "        if idx % save_every == 0 or idx == len(sample_df):\n",
    "            df_partial = pd.DataFrame(results)\n",
    "            df_partial.to_csv(os.path.join(OUTPUT_DIR, \"analysis_results_partial.csv\"), index=False)\n",
    "            with open(os.path.join(OUTPUT_DIR, \"analysis_results_partial.json\"), \"w\") as fh:\n",
    "                json.dump(results, fh, indent=2)\n",
    "            logging.info(f\"Saved partial results at {idx}/{len(sample_df)}\")\n",
    "\n",
    "    # Save final, full outputs\n",
    "    df_final = pd.DataFrame(results)\n",
    "    df_final.to_csv(os.path.join(OUTPUT_DIR, \"analysis_results_offline.csv\"), index=False)\n",
    "    df_final.to_json(os.path.join(OUTPUT_DIR, \"analysis_results_offline.json\"), orient=\"records\", indent=2)\n",
    "\n",
    "    # Build ATT&CK Navigator layer\n",
    "    layer_json = build_attck_layer(tech_counter)\n",
    "    with open(os.path.join(OUTPUT_DIR, \"attack_navigator_layer.json\"), \"w\") as fh:\n",
    "        json.dump(layer_json, fh, indent=2)\n",
    "\n",
    "    logging.info(\"Offline collection complete. Outputs saved to ./output\")\n",
    "    return results\n",
    "\n",
    "print(\"✅ Collector ready (offline)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06e3eed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Main ready (offline)\n"
     ]
    }
   ],
   "source": [
    "def main(input_csv: str = INPUT_CSV,\n",
    "         min_samples: int = TARGET_MIN_SAMPLES,\n",
    "         max_samples: int = TARGET_MAX_SAMPLES,\n",
    "         min_families: int = MIN_FAMILIES):\n",
    "    \"\"\"Offline entry point that operates entirely on cached JSON files.\"\"\"\n",
    "    logging.info(\"Reading input CSV...\")\n",
    "    df = read_hash_csv(input_csv)\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"No hashes found in input CSV. Exiting.\")\n",
    "\n",
    "    assert_dataset_meets_requirements(df, min_samples=min_samples, min_families=min_families)\n",
    "\n",
    "    samples = select_sample_hashes(df, min_samples=min_samples, max_samples=max_samples, min_families=min_families)\n",
    "    logging.info(f\"Beginning offline aggregation for {len(samples)} samples using cached responses in {RAW_DIR}\")\n",
    "\n",
    "    results = collect_samples(samples)\n",
    "\n",
    "    df_res = pd.DataFrame(results)\n",
    "    success_count = (df_res['status'] == 'success').sum()\n",
    "    logging.info(f\"Finished. Success: {success_count}/{len(df_res)}\")\n",
    "    logging.info(\"Open output/attack_navigator_layer.json in ATT&CK Navigator to visualize technique coverage.\")\n",
    "\n",
    "print(\"✅ Main ready (offline)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed09d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MITRE mapping helper ready\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "def generate_mitre_mapping(results_path: str = \"./output/analysis_results_offline.json\",\n",
    "                            attack_bundle_path: str = \"./enterprise-attack.json\",\n",
    "                            mapping_output_path: str = \"./output/mitre_technique_mapping.json\") -> Dict[str, Any]:\n",
    "    \"\"\"Map deprecated MITRE technique IDs to their replacements using cached offline results.\"\"\"\n",
    "    results_file = Path(results_path)\n",
    "    attack_file = Path(attack_bundle_path)\n",
    "    if not results_file.exists():\n",
    "        raise FileNotFoundError(f\"Analysis file not found: {results_file}\")\n",
    "    if not attack_file.exists():\n",
    "        raise FileNotFoundError(f\"ATT&CK bundle not found: {attack_file}\")\n",
    "\n",
    "    def load_json_payload(path: Path) -> Any:\n",
    "        with path.open('r', encoding='utf-8') as handle:\n",
    "            return json.load(handle)\n",
    "\n",
    "    def split_technique_list(raw: str) -> List[str]:\n",
    "        if not raw:\n",
    "            return []\n",
    "        tokens = re.split(r'[|,]+', raw)\n",
    "        return [token.strip() for token in tokens if token and token.strip()]\n",
    "\n",
    "    def build_attack_maps(attack_stix: Dict[str, Any]) -> Tuple[Dict[str, str], Dict[str, str], Dict[str, List[Tuple[str, str]]]]:\n",
    "        extid_to_objid: Dict[str, str] = {}\n",
    "        objid_to_extid: Dict[str, str] = {}\n",
    "        rel_from_to: Dict[str, List[Tuple[str, str]]] = defaultdict(list)\n",
    "        for obj in attack_stix.get('objects', []):\n",
    "            if obj.get('type') != 'attack-pattern':\n",
    "                continue\n",
    "            external_id = None\n",
    "            for ref in obj.get('external_references', []):\n",
    "                if ref.get('source_name') == 'mitre-attack' and ref.get('external_id', '').startswith('T'):\n",
    "                    external_id = ref['external_id']\n",
    "                    break\n",
    "            if not external_id:\n",
    "                continue\n",
    "            obj_id = obj.get('id')\n",
    "            extid_to_objid[external_id] = obj_id\n",
    "            objid_to_extid[obj_id] = external_id\n",
    "        for obj in attack_stix.get('objects', []):\n",
    "            if obj.get('type') != 'relationship':\n",
    "                continue\n",
    "            rtype = (obj.get('relationship_type') or '').lower()\n",
    "            if not rtype or not any(keyword in rtype for keyword in ('replace', 'revoked', 'duplicate')):\n",
    "                continue\n",
    "            src = obj.get('source_ref')\n",
    "            tgt = obj.get('target_ref')\n",
    "            if src and tgt:\n",
    "                rel_from_to[src].append((tgt, rtype))\n",
    "        return extid_to_objid, objid_to_extid, rel_from_to\n",
    "\n",
    "    def find_replacement_extids(extid: str,\n",
    "                                extid_to_objid: Dict[str, str],\n",
    "                                objid_to_extid: Dict[str, str],\n",
    "                                rel_from_to: Dict[str, List[Tuple[str, str]]]) -> List[Tuple[str, str]]:\n",
    "        replacements: List[Tuple[str, str]] = []\n",
    "        obj_id = extid_to_objid.get(extid)\n",
    "        if not obj_id:\n",
    "            return replacements\n",
    "        for tgt_oid, rtype in rel_from_to.get(obj_id, []):\n",
    "            tgt_ext = objid_to_extid.get(tgt_oid)\n",
    "            if tgt_ext:\n",
    "                replacements.append((tgt_ext, rtype))\n",
    "        for src_oid, targets in rel_from_to.items():\n",
    "            for tgt_oid, rtype in targets:\n",
    "                if tgt_oid == obj_id:\n",
    "                    src_ext = objid_to_extid.get(src_oid)\n",
    "                    if src_ext and (src_ext, rtype) not in replacements:\n",
    "                        replacements.append((src_ext, rtype))\n",
    "        seen: set[str] = set()\n",
    "        ordered: List[Tuple[str, str]] = []\n",
    "        for ext, rel in replacements:\n",
    "            if ext not in seen:\n",
    "                ordered.append((ext, rel))\n",
    "                seen.add(ext)\n",
    "        return ordered\n",
    "\n",
    "    results_payload = load_json_payload(results_file)\n",
    "    attack_bundle = load_json_payload(attack_file)\n",
    "    extid_to_objid, objid_to_extid, rel_from_to = build_attack_maps(attack_bundle)\n",
    "\n",
    "    unique_techniques: set[str] = set()\n",
    "    for record in results_payload:\n",
    "        unique_techniques.update(split_technique_list(record.get('mitre_techniques', '')))\n",
    "\n",
    "    mapping: Dict[str, Dict[str, Any]] = {}\n",
    "    for tid in sorted(unique_techniques):\n",
    "        replacements = find_replacement_extids(tid, extid_to_objid, objid_to_extid, rel_from_to)\n",
    "        if not replacements:\n",
    "            continue\n",
    "        primary = replacements[0][0]\n",
    "        if primary == tid:\n",
    "            continue\n",
    "        mapping[tid] = {\n",
    "            'replacement': primary,\n",
    "            'relationships': [\n",
    "                {'technique_id': rep_id, 'relationship': rel or ''}\n",
    "                for rep_id, rel in replacements\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    output_path = Path(mapping_output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_path.open('w', encoding='utf-8') as handle:\n",
    "        json.dump(mapping, handle, indent=2)\n",
    "\n",
    "    logging.info(\"Identified %d unique MITRE techniques (%d replacements)\", len(unique_techniques), len(mapping))\n",
    "    logging.info(\"Saved mapping → %s\", output_path)\n",
    "    return mapping\n",
    "\n",
    "print(\"✅ MITRE mapping helper ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44acb778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MITRE mapping application helper ready\n"
     ]
    }
   ],
   "source": [
    "def apply_mitre_mapping_to_offline_results(mapping_path: str = \"./output/mitre_technique_mapping.json\",\n",
    "                                            json_path: str = \"./output/analysis_results_offline.json\",\n",
    "                                            csv_path: str = \"./output/analysis_results_offline.csv\") -> pd.DataFrame:\n",
    "    \"\"\"Copy current MITRE IDs to `mitre_technique_old` and append replacements alongside existing IDs.\"\"\"\n",
    "    mapping_file = Path(mapping_path)\n",
    "    json_file = Path(json_path)\n",
    "    csv_file = Path(csv_path)\n",
    "\n",
    "    if not mapping_file.exists():\n",
    "        raise FileNotFoundError(f\"Mapping file not found: {mapping_file}\")\n",
    "    if not json_file.exists():\n",
    "        raise FileNotFoundError(f\"Offline JSON results not found: {json_file}\")\n",
    "    if not csv_file.exists():\n",
    "        raise FileNotFoundError(f\"Offline CSV results not found: {csv_file}\")\n",
    "\n",
    "    with mapping_file.open('r', encoding='utf-8') as handle:\n",
    "        mapping = json.load(handle)\n",
    "\n",
    "    with json_file.open('r', encoding='utf-8') as handle:\n",
    "        records = json.load(handle)\n",
    "\n",
    "    def split_techniques(raw: str) -> List[str]:\n",
    "        if not raw:\n",
    "            return []\n",
    "        return [tid.strip() for tid in raw.split(',') if tid.strip()]\n",
    "\n",
    "    def join_unique(values: List[str]) -> str:\n",
    "        ordered: List[str] = []\n",
    "        seen: set[str] = set()\n",
    "        for tid in values:\n",
    "            if tid and tid not in seen:\n",
    "                ordered.append(tid)\n",
    "                seen.add(tid)\n",
    "        return ', '.join(ordered)\n",
    "\n",
    "    updated_records: List[Dict[str, Any]] = []\n",
    "    replacements_applied = 0\n",
    "\n",
    "    for record in records:\n",
    "        original = record.get('mitre_techniques', '') or ''\n",
    "        techniques = split_techniques(original)\n",
    "        combined: List[str] = []\n",
    "        for tid in techniques:\n",
    "            combined.append(tid)\n",
    "            remap = mapping.get(tid)\n",
    "            if remap:\n",
    "                new_tid = remap['replacement']\n",
    "                combined.append(new_tid)\n",
    "                replacements_applied += 1\n",
    "        record['mitre_technique_old'] = original\n",
    "        record['mitre_techniques'] = join_unique(combined)\n",
    "        record['mitre_techniques_count'] = len(split_techniques(record['mitre_techniques']))\n",
    "        updated_records.append(record)\n",
    "\n",
    "    with json_file.open('w', encoding='utf-8') as handle:\n",
    "        json.dump(updated_records, handle, indent=2)\n",
    "\n",
    "    df = pd.DataFrame(updated_records)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "    logging.info(\"Updated %d records; appended %d replacement techniques\", len(updated_records), replacements_applied)\n",
    "    logging.info(\"JSON updated → %s\", json_file)\n",
    "    logging.info(\"CSV updated → %s\", csv_file)\n",
    "    return df\n",
    "\n",
    "print(\"✅ MITRE mapping application helper ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283988b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 14:30:48,106 - INFO - Reading input CSV...\n",
      "2025-11-10 14:30:48,122 - INFO - Available families in input: 23\n",
      "2025-11-10 14:30:48,122 - INFO - Target sample count: 100\n",
      "2025-11-10 14:30:48,139 - INFO - Selected 100 samples from 23 families\n",
      "2025-11-10 14:30:48,140 - INFO - Beginning offline aggregation for 100 samples using cached responses in ./output/raw_responses\n",
      "Parsing cached samples:   0%|                                                                                                                                                                                         | 0/100 [00:00<?, ?it/s]2025-11-10 14:30:48,155 - WARNING - Skipping 818c482b0d6be6f5c9449c76d79edf4e038fe639267b2da83675e0c5b723cea7: failed to read cached file response: Expecting value: line 1 column 1 (char 0)\n",
      "2025-11-10 14:30:48,173 - INFO - Saved partial results at 10/100\n",
      "2025-11-10 14:30:48,194 - INFO - Saved partial results at 20/100\n",
      "2025-11-10 14:30:48,209 - INFO - Saved partial results at 30/100\n",
      "2025-11-10 14:30:48,226 - INFO - Saved partial results at 40/100\n",
      "2025-11-10 14:30:48,244 - INFO - Saved partial results at 50/100\n",
      "Parsing cached samples:  56%|██████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 56/100 [00:00<00:00, 549.25it/s]2025-11-10 14:30:48,263 - INFO - Saved partial results at 60/100\n",
      "2025-11-10 14:30:48,280 - INFO - Saved partial results at 70/100\n",
      "2025-11-10 14:30:48,296 - INFO - Saved partial results at 80/100\n",
      "2025-11-10 14:30:48,340 - INFO - Saved partial results at 90/100\n",
      "2025-11-10 14:30:48,355 - INFO - Saved partial results at 100/100\n",
      "Parsing cached samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 498.18it/s]\n",
      "2025-11-10 14:30:48,358 - INFO - Offline collection complete. Outputs saved to ./output\n",
      "2025-11-10 14:30:48,359 - INFO - Finished. Success: 99/100\n",
      "2025-11-10 14:30:48,359 - INFO - Open output/attack_navigator_layer.json in ATT&CK Navigator to visualize technique coverage.\n"
     ]
    }
   ],
   "source": [
    "# Run the offline flow when you're ready.\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faeb7d60-1474-4e1a-8ca3-2077442fd6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MITRE ATT&CK data from local file: enterprise-attack.json...\n",
      "\n",
      "Successfully generated ATT&CK Layer file: 'output/attack_layer_offline.json' with authoritative mapping.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import os  # <-- New import\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from stix2 import MemoryStore\n",
    "from stix2 import Filter  # Need this for stix2 queries\n",
    "\n",
    "# --- Configuration ---\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "INPUT_FILE = output_dir / 'analysis_results_offline.json'\n",
    "OUTPUT_FILE = output_dir / 'attack_layer_offline.json'\n",
    "ATTACK_STIX_URL = \"https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\"\n",
    "LOCAL_STIX_FILE = \"enterprise-attack.json\"  # <-- New setting\n",
    "LAYER_NAME = \"Malware Behavioral Layer (Authoritative Mapping)\"\n",
    "MAX_SCORE_CAP = 100\n",
    "\n",
    "def parse_technique_ids(raw_value):\n",
    "    \"\"\"Split MITRE technique strings regardless of the delimiter used.\"\"\"\n",
    "    if not raw_value:\n",
    "        return []\n",
    "\n",
    "    if isinstance(raw_value, list):\n",
    "        tokens = raw_value\n",
    "    else:\n",
    "        tokens = re.split(r'[|,]+', raw_value)\n",
    "\n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    for token in tokens:\n",
    "        tid = (token or '').strip()\n",
    "        if not tid or tid in seen:\n",
    "            continue\n",
    "        cleaned.append(tid)\n",
    "        seen.add(tid)\n",
    "    return cleaned\n",
    "\n",
    "def fetch_mitre_stix_data(url, local_file):\n",
    "    \"\"\"Fetches the latest MITRE ATT&CK STIX data, prioritizing local file load.\"\"\"\n",
    "    \n",
    "    # 1. Try to load from a local file first\n",
    "    if os.path.exists(local_file):\n",
    "        print(f\"Loading MITRE ATT&CK data from local file: {local_file}...\")\n",
    "        try:\n",
    "            with open(local_file, 'r', encoding='utf-8') as f:\n",
    "                stix_data = json.load(f)\n",
    "            # stix2.MemoryStore expects the 'objects' list\n",
    "            return MemoryStore(stix_data=stix_data.get(\"objects\", []))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local file '{local_file}': {e}\")\n",
    "    \n",
    "    # 2. Fallback to network request if local file is not found or fails\n",
    "    print(\"Local file not found or failed to load. Attempting to fetch data over network...\")\n",
    "    try:\n",
    "        # Increase timeout to 60 seconds for the large file\n",
    "        response = requests.get(url, timeout=60) \n",
    "        response.raise_for_status() \n",
    "        stix_data = response.json()\n",
    "        print(\"Successfully downloaded MITRE ATT&CK data.\")\n",
    "        return MemoryStore(stix_data=stix_data.get(\"objects\", []))\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching MITRE data from URL: {e}\")\n",
    "        print(\"Please ensure you have a stable network connection.\")\n",
    "        return None\n",
    "\n",
    "def build_accurate_tactic_map(stix_store):\n",
    "    \"\"\"\n",
    "    Builds a dictionary {technique_id: [tactic_names]} using the STIX relationships.\n",
    "    \"\"\"\n",
    "    if not stix_store:\n",
    "        return {}\n",
    "        \n",
    "    tactic_map = defaultdict(list)\n",
    "    \n",
    "    # 1. Get all Techniques and Sub-techniques (type='attack-pattern')\n",
    "    techniques = stix_store.query(Filter(\"type\", \"=\", \"attack-pattern\"))\n",
    "    \n",
    "    # 2. Map Techniques to Tactics\n",
    "    for tech in techniques:\n",
    "        # Get the primary ATT&CK ID (e.g., T1560)\n",
    "        tech_id = tech.external_references[0]['external_id']\n",
    "        \n",
    "        # Techniques can belong to multiple tactics\n",
    "        if hasattr(tech, 'kill_chain_phases'):\n",
    "            for phase in tech.kill_chain_phases:\n",
    "                # The 'phase_name' corresponds to the Tactic's short name (e.g., initial-access)\n",
    "                tactic_name = phase['phase_name'].lower().replace(' ', '-')\n",
    "                tactic_map[tech_id].append(tactic_name)\n",
    "    \n",
    "    return {k: sorted(list(set(v))) for k, v in tactic_map.items()}\n",
    "\n",
    "# ------------------------------------\n",
    "\n",
    "def generate_attack_layer(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reads the analysis file, aggregates MITRE techniques using official mapping, \n",
    "    and generates the layer JSON.\n",
    "    \"\"\"\n",
    "    # 0. Set up the authoritative mapping\n",
    "    stix_store = fetch_mitre_stix_data(ATTACK_STIX_URL, LOCAL_STIX_FILE)\n",
    "    if not stix_store:\n",
    "        print(\"Cannot proceed without MITRE ATT&CK data. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    TECHNIQUE_TO_TACTIC_MAP = build_accurate_tactic_map(stix_store)\n",
    "    \n",
    "    # 1. Load your analysis data\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            analysis_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading input file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Dictionary to store technique ID -> set of families that used it\n",
    "    technique_family_map = defaultdict(set)\n",
    "    \n",
    "    # 2. Aggregate Techniques and Families (Same as before)\n",
    "    for sample in analysis_data:\n",
    "        family = sample.get(\"family\", \"unknown_family\")\n",
    "        techniques = parse_technique_ids(sample.get(\"mitre_techniques\"))\n",
    "        \n",
    "        for technique_id in techniques:\n",
    "            technique_family_map[technique_id].add(family)\n",
    "\n",
    "    if not technique_family_map:\n",
    "        print(\"No MITRE techniques were found in the offline analysis file.\")\n",
    "        return\n",
    "\n",
    "    # 3. Construct Layer Data using the AUTHORITATIVE MAP\n",
    "    max_family_count = 0\n",
    "    layer_techniques = []\n",
    "\n",
    "    for technique_id, families in sorted(technique_family_map.items()):\n",
    "        family_count = len(families)\n",
    "        max_family_count = max(max_family_count, family_count)\n",
    "        \n",
    "        # Get Tactic(s) from the official map\n",
    "        official_tactics = TECHNIQUE_TO_TACTIC_MAP.get(technique_id, [\"unknown-tactic\"])\n",
    "        \n",
    "        # Use the first tactic for the primary field\n",
    "        primary_tactic = official_tactics[0] \n",
    "        \n",
    "        # Simple color assignment based on known tactics\n",
    "        color = \"#6c0bf4\"  # Default color (gray)\n",
    "        if \"execution\" in official_tactics:\n",
    "            color = \"#ff6666\"  # Red\n",
    "        elif \"persistence\" in official_tactics:\n",
    "            color = \"#cc99ff\"  # Purple\n",
    "        elif \"defense-evasion\" in official_tactics:\n",
    "            color = \"#ffcc66\"  # Yellow/Orange\n",
    "\n",
    "        layer_techniques.append({\n",
    "            \"techniqueID\": technique_id,\n",
    "            \"tactic\": primary_tactic,\n",
    "            \"score\": family_count,\n",
    "            \"color\": color,\n",
    "            \"metadata\": [\n",
    "                {\"name\": \"Family Count\", \"value\": str(family_count)},\n",
    "                {\"name\": \"Official Tactics\", \"value\": \", \".join(t.replace('-', ' ').title() for t in official_tactics)}\n",
    "            ],\n",
    "            \"comment\": f\"Used by {family_count} families. Official Tactic(s): {', '.join(t.replace('-', ' ').title() for t in official_tactics)}.\"\n",
    "        })\n",
    "\n",
    "    # 4. Final Layer Construction\n",
    "    layer_json = {\n",
    "        \"name\": LAYER_NAME,\n",
    "        \"description\": f\"Aggregated Techniques from {len(analysis_data)} samples.\",\n",
    "        \"domain\": \"enterprise-attack\",\n",
    "        \"version\": \"4.5\",\n",
    "        \"techniques\": layer_techniques,\n",
    "        \"gradient\": {\n",
    "            \"colors\": [\"#ffffff\", \"#ff0000\"],\n",
    "            \"minValue\": 0,\n",
    "            \"maxValue\": MAX_SCORE_CAP if max_family_count < MAX_SCORE_CAP else max_family_count + 5 \n",
    "        },\n",
    "        \"hideDisabled\": False,\n",
    "        \"showTacticRowBackground\": True\n",
    "    }\n",
    "    \n",
    "    # 5. Save the Layer JSON\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(layer_json, f, indent=2)\n",
    "        print(f\"\\nSuccessfully generated ATT&CK Layer file: '{output_file}' with authoritative mapping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate_attack_layer(INPUT_FILE, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15faf52-60a4-40b0-9faf-8bf273d3ebed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
