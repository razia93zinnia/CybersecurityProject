{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24beda0d",
   "metadata": {},
   "source": [
    "# VirusTotal â†’ MITRE ATT&CK Pipeline (Grad-level)\n",
    "This notebook collects VirusTotal (VT) file + behaviour summaries for a selected set of malware hashes,\n",
    "maps behaviours to MITRE ATT&CK techniques, and exports results for analysis and ATT&CK Navigator.\n",
    "\n",
    "**What you'll get:**\n",
    "- `output/analysis_results.csv` and `.json` (per-sample indicators and technique lists)\n",
    "- `output/raw_responses/` (raw VT responses for audit)\n",
    "- `output/attck_navigator_layer.json` (import into ATT&CK Navigator)\n",
    "\n",
    "**Grad requirements enforced:**\n",
    "- â‰¥ 75 samples\n",
    "- â‰¥ 6 malware families\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5475d1",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 2 â€” Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e7b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports ready\n"
     ]
    }
   ],
   "source": [
    "# Core / stdlib\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime, timezone\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party libs\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optional: load .env (so you can store VIRUSTOTAL_API_KEY there)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"âœ… Imports ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00cc79",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 3 â€” Configuration (edit here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1443dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Config ready\n"
     ]
    }
   ],
   "source": [
    "# ---- USER-EDITABLE CONFIG ----\n",
    "\n",
    "# 1) API key: set via env var or .env file (recommended)\n",
    "API_KEY = os.getenv(\"VIRUSTOTAL_API_KEY\")  # DO NOT hardcode in code you share\n",
    "if not API_KEY:\n",
    "    raise SystemExit(\"ERROR: Set environment variable VIRUSTOTAL_API_KEY or use a .env file.\")\n",
    "\n",
    "# 2) I/O paths\n",
    "OUTPUT_DIR = \"./output\"\n",
    "RAW_DIR = os.path.join(OUTPUT_DIR, \"raw_responses\")\n",
    "INPUT_CSV = \"./output/hash_signature_output.csv\"   # change to your CSV path if needed\n",
    "\n",
    "# 3) VT API base and headers\n",
    "VT_BASE = \"https://www.virustotal.com/api/v3\"\n",
    "HEADERS = {\"x-apikey\": API_KEY}\n",
    "\n",
    "# 4) Rate limits â€” VT Free â‰ˆ 4 req/min â‡’ 15s between requests; stay conservative.\n",
    "DELAY_BETWEEN_REQUESTS = 16.0\n",
    "\n",
    "# 5) Assignment targets (Grad)\n",
    "TARGET_MIN_SAMPLES = 78    # minimum total samples to collect\n",
    "TARGET_MAX_SAMPLES = 100   # cap to avoid over-collection\n",
    "MIN_FAMILIES = 8           # at least 6 malware families\n",
    "\n",
    "# 6) Enforce minimums strictly (exit early if unmet)\n",
    "HARD_REQUIRE_MINIMUMS = True\n",
    "\n",
    "# Logging (INFO is good; use DEBUG for even more detail)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Ensure output folders exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Config ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8b850",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 4 â€” HTTP Session with Retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2399d479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HTTP session ready with retries\n"
     ]
    }
   ],
   "source": [
    "# Create a single requests.Session with retry policy:\n",
    "# - Retries on 429 (rate limit) and common 5xx errors\n",
    "# - Exponential backoff between attempts\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"]\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "print(\"âœ… HTTP session ready with retries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495720ff",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 5 â€” CSV Reader & Basic Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a097bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSV helpers ready\n"
     ]
    }
   ],
   "source": [
    "def read_hash_csv(csv_path: str, hash_col: str = \"hash\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read hashes + metadata from a CSV.\n",
    "    Required column: 'hash'\n",
    "    Optional columns: 'malware_family', 'source'\n",
    "    - Deduplicates by 'hash'\n",
    "    - Lowercases and trims hash strings\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, dtype=str).fillna(\"\")\n",
    "    if hash_col not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain a '{hash_col}' column. Found: {list(df.columns)}\")\n",
    "    df['hash'] = df['hash'].str.strip().str.lower()\n",
    "\n",
    "    if 'malware_family' not in df.columns:\n",
    "        df['malware_family'] = \"Unknown\"\n",
    "    if 'source' not in df.columns:\n",
    "        df['source'] = \"Unknown\"\n",
    "\n",
    "    df = df[df['hash'] != \"\"].drop_duplicates(subset=['hash'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def assert_dataset_meets_requirements(df: pd.DataFrame,\n",
    "                                      min_samples: int,\n",
    "                                      min_families: int) -> None:\n",
    "    \"\"\"\n",
    "    Hard stop (or warn) if the dataset does not meet min counts.\n",
    "    - Unique hashes â‰¥ min_samples\n",
    "    - Family count â‰¥ min_families\n",
    "    \"\"\"\n",
    "    num_hashes = df['hash'].nunique()\n",
    "    num_fams = df['malware_family'].replace(\"\", \"Unknown\").nunique()\n",
    "\n",
    "    problems = []\n",
    "    if num_hashes < min_samples:\n",
    "        problems.append(f\"- Only {num_hashes} unique hashes found (need â‰¥ {min_samples}).\")\n",
    "    if num_fams < min_families:\n",
    "        problems.append(f\"- Only {num_fams} malware families found (need â‰¥ {min_families}).\")\n",
    "\n",
    "    if problems and HARD_REQUIRE_MINIMUMS:\n",
    "        msg = (\n",
    "            \"\\nINPUT DATA DOES NOT MEET MINIMUMS:\\n\"\n",
    "            + \"\\n\".join(problems)\n",
    "            + \"\\n\\nTips:\\n\"\n",
    "              \"  â€¢ Add hashes from MalwareBazaar / Malpedia / ANY.RUN.\\n\"\n",
    "              \"  â€¢ Ensure 'hash' is valid and 'malware_family' is labeled.\\n\"\n",
    "              \"  â€¢ Remove duplicates (one row per unique hash).\\n\"\n",
    "        )\n",
    "        raise SystemExit(msg)\n",
    "    elif problems:\n",
    "        logging.warning(\"Proceeding despite dataset not meeting minimums:\\n\" + \"\\n\".join(problems))\n",
    "\n",
    "print(\"âœ… CSV helpers ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce47b6c",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 6 â€” Balanced, Round-Robin Sampling (â‰¥75 samples & â‰¥6 families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62e53d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sampler ready\n"
     ]
    }
   ],
   "source": [
    "def select_sample_hashes(df: pd.DataFrame,\n",
    "                         min_samples: int = TARGET_MIN_SAMPLES,\n",
    "                         max_samples: int = TARGET_MAX_SAMPLES,\n",
    "                         min_families: int = MIN_FAMILIES) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a balanced selection by iterating families in round-robin until we hit the target.\n",
    "    Ensures (best effort) â‰¥ min_families in the final selection.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['malware_family'] = df['malware_family'].replace(\"\", \"Unknown\")\n",
    "\n",
    "    # Enforce minimums on the whole dataset\n",
    "    assert_dataset_meets_requirements(df, min_samples=min_samples, min_families=min_families)\n",
    "\n",
    "    # Families ordered by frequency (more populous first)\n",
    "    families_ordered = df['malware_family'].value_counts().index.tolist()\n",
    "    logging.info(f\"Available families in input: {len(families_ordered)}\")\n",
    "\n",
    "    # Target amount (prefer min_samples, never exceed max_samples or dataset size)\n",
    "    target = min(max(min_samples, min(len(df), max_samples)), max_samples)\n",
    "    target = min(target, len(df))\n",
    "    logging.info(f\"Target sample count: {target}\")\n",
    "\n",
    "    # Family pools shuffled deterministically for reproducibility\n",
    "    pools: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for fam in families_ordered:\n",
    "        pools[fam] = df[df['malware_family'] == fam].sample(frac=1.0, random_state=42).to_dict(orient='records')\n",
    "\n",
    "    # Round-robin pick across families\n",
    "    selected: List[Dict[str, Any]] = []\n",
    "    families_rr = families_ordered[:]\n",
    "    fam_idx = 0\n",
    "    while len(selected) < target and families_rr:\n",
    "        fam = families_rr[fam_idx % len(families_rr)]\n",
    "        if pools[fam]:\n",
    "            selected.append(pools[fam].pop())\n",
    "            fam_idx += 1\n",
    "        else:\n",
    "            # remove empty families from rotation\n",
    "            families_rr.pop(fam_idx % len(families_rr))\n",
    "\n",
    "    sampled_df = pd.DataFrame(selected)\n",
    "\n",
    "    # Ensure â‰¥ min_families in the final selection (swap in if needed)\n",
    "    sel_fams = sampled_df['malware_family'].nunique()\n",
    "    if sel_fams < min_families:\n",
    "        logging.warning(f\"Selection has only {sel_fams} families; trying to add more...\")\n",
    "        remaining_rows = df[~df['hash'].isin(sampled_df['hash'])]\n",
    "        extras = (remaining_rows.groupby('malware_family').head(1).reset_index(drop=True))\n",
    "        for _, row in extras.iterrows():\n",
    "            if row['malware_family'] not in sampled_df['malware_family'].unique():\n",
    "                if len(sampled_df) < target:\n",
    "                    sampled_df = pd.concat([sampled_df, pd.DataFrame([row])], ignore_index=True)\n",
    "                else:\n",
    "                    # Replace one row from most common family to keep total unchanged\n",
    "                    over_fam = sampled_df['malware_family'].value_counts().index[0]\n",
    "                    idx_drop = sampled_df[sampled_df['malware_family'] == over_fam].index[0]\n",
    "                    sampled_df = sampled_df.drop(index=idx_drop)\n",
    "                    sampled_df = pd.concat([sampled_df, pd.DataFrame([row])], ignore_index=True)\n",
    "            if sampled_df['malware_family'].nunique() >= min_families:\n",
    "                break\n",
    "\n",
    "    logging.info(f\"Selected {len(sampled_df)} samples from {sampled_df['malware_family'].nunique()} families\")\n",
    "    return sampled_df.reset_index(drop=True)\n",
    "\n",
    "print(\"âœ… Sampler ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52074a76",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 7 â€” VirusTotal GET Helper (with backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab45285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VT API helpers ready\n"
     ]
    }
   ],
   "source": [
    "def vt_get(endpoint: str, params: Optional[Dict] = None, max_retries: int = 4) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    GET helper with exponential backoff on 429/5xx.\n",
    "    Returns: {'success': bool, 'status_code': int|None, 'json': dict|None, 'error': str|None}\n",
    "    \"\"\"\n",
    "    url = f\"{VT_BASE}{endpoint}\"\n",
    "    attempt = 0\n",
    "    backoff = 1.0\n",
    "\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            resp = session.get(url, headers=HEADERS, params=params, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                return {'success': True, 'status_code': 200, 'json': resp.json()}\n",
    "            if resp.status_code == 404:\n",
    "                return {'success': False, 'status_code': 404, 'error': 'Not found'}\n",
    "            if resp.status_code == 429:\n",
    "                logging.warning(f\"429 rate limit; sleeping {backoff:.1f}s (attempt {attempt})\")\n",
    "                time.sleep(backoff); backoff *= 2; attempt += 1; continue\n",
    "            if 500 <= resp.status_code < 600:\n",
    "                logging.warning(f\"Server error {resp.status_code}; sleeping {backoff:.1f}s (attempt {attempt})\")\n",
    "                time.sleep(backoff); backoff *= 2; attempt += 1; continue\n",
    "            # other non-success\n",
    "            try:\n",
    "                j = resp.json()\n",
    "            except Exception:\n",
    "                j = None\n",
    "            return {'success': False, 'status_code': resp.status_code, 'json': j, 'error': f\"HTTP {resp.status_code}\"}\n",
    "        except requests.RequestException as e:\n",
    "            logging.warning(f\"Request error: {e}; sleeping {backoff:.1f}s (attempt {attempt})\")\n",
    "            time.sleep(backoff); backoff *= 2; attempt += 1\n",
    "\n",
    "    return {'success': False, 'status_code': None, 'error': 'Max retries exceeded'}\n",
    "\n",
    "\n",
    "def get_file_report(hash_val: str) -> Dict[str, Any]:\n",
    "    \"\"\"VT file metadata (detections, timestamps, etc.).\"\"\"\n",
    "    return vt_get(f\"/files/{hash_val}\")\n",
    "\n",
    "\n",
    "def get_behavior_report(hash_val: str) -> Dict[str, Any]:\n",
    "    \"\"\"VT sandbox behaviour summary when available.\"\"\"\n",
    "    return vt_get(f\"/files/{hash_val}/behaviour_summary\")\n",
    "\n",
    "print(\"âœ… VT API helpers ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62c653",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 8 â€” Safe Behaviour Parsing (null-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94a51df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Behaviour parser ready\n"
     ]
    }
   ],
   "source": [
    "def extract_behavioral_indicators(behav_resp: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse /behaviour_summary response safely into flattened indicators.\n",
    "    Returns empty lists when fields are missing (some hashes have no behaviour).\n",
    "    \"\"\"\n",
    "    indicators = {\n",
    "        'processes_created': [],\n",
    "        'files_written': [],\n",
    "        'files_deleted': [],\n",
    "        'registry_keys_set': [],\n",
    "        'registry_keys_deleted': [],\n",
    "        'dns_lookups': [],\n",
    "        'ip_traffic': [],\n",
    "        'http_conversations': [],\n",
    "        'command_executions': [],\n",
    "        'mutexes_created': [],\n",
    "        'services_created': [],\n",
    "        'mitre_techniques': []\n",
    "    }\n",
    "\n",
    "    # If HTTP call failed or no JSON, return empty indicators\n",
    "    if not behav_resp or not behav_resp.get('success'):\n",
    "        return indicators\n",
    "\n",
    "    try:\n",
    "        j = behav_resp.get('json') or {}\n",
    "        if not isinstance(j, dict):\n",
    "            return indicators\n",
    "\n",
    "        # Expected nesting: json['data']['data']['attributes']\n",
    "        data1 = j.get('data') or {}\n",
    "        data2 = data1.get('data') if isinstance(data1, dict) else {}\n",
    "        attrs = (data2.get('attributes') if isinstance(data2, dict) else {}) or {}\n",
    "        if not isinstance(attrs, dict):\n",
    "            return indicators\n",
    "\n",
    "        # Copy simple lists\n",
    "        indicators['processes_created'] = attrs.get('processes_created') or []\n",
    "        indicators['files_written']     = attrs.get('files_written')     or []\n",
    "        indicators['files_deleted']     = attrs.get('files_deleted')     or []\n",
    "        indicators['registry_keys_set'] = attrs.get('registry_keys_set') or []\n",
    "        indicators['registry_keys_deleted'] = attrs.get('registry_keys_deleted') or []\n",
    "        indicators['command_executions']    = attrs.get('command_executions')    or []\n",
    "        indicators['mutexes_created']       = attrs.get('mutexes_created')       or []\n",
    "        indicators['services_created']      = attrs.get('services_created')      or []\n",
    "\n",
    "        # Normalize structured items\n",
    "        dns_lookups = attrs.get('dns_lookups') or []\n",
    "        if isinstance(dns_lookups, list):\n",
    "            indicators['dns_lookups'] = [d.get('hostname', '') for d in dns_lookups if isinstance(d, dict)]\n",
    "\n",
    "        ip_traffic = attrs.get('ip_traffic') or []\n",
    "        if isinstance(ip_traffic, list):\n",
    "            indicators['ip_traffic'] = [\n",
    "                f\"{ip.get('destination_ip','')}:{ip.get('destination_port','')}\"\n",
    "                for ip in ip_traffic if isinstance(ip, dict)\n",
    "            ]\n",
    "\n",
    "        http_conversations = attrs.get('http_conversations') or []\n",
    "        if isinstance(http_conversations, list):\n",
    "            indicators['http_conversations'] = [h.get('url','') for h in http_conversations if isinstance(h, dict)]\n",
    "\n",
    "        mt = attrs.get('mitre_attack_techniques') or []\n",
    "        if isinstance(mt, list):\n",
    "            indicators['mitre_techniques'] = [t for t in mt if isinstance(t, dict)]\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing behavior response safely: {e}\")\n",
    "\n",
    "    return indicators\n",
    "\n",
    "print(\"âœ… Behaviour parser ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61077de5",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 9 â€” Build ATT&CK Navigator Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab308156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ATT&CK layer builder ready\n"
     ]
    }
   ],
   "source": [
    "def build_attck_layer(tech_counter: Counter) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a minimal ATT&CK Navigator layer.\n",
    "    - technique.score = occurrence count across all samples\n",
    "    - tactic left blank (behaviour_summary may not map tactics)\n",
    "    \"\"\"\n",
    "    techniques = [\n",
    "        {\"techniqueID\": tid, \"tactic\": \"\", \"score\": int(count)}\n",
    "        for tid, count in tech_counter.items()\n",
    "    ]\n",
    "    return {\n",
    "        \"name\": \"VT â†’ MITRE ATT&CK Layer\",\n",
    "        \"description\": \"Techniques extracted from VirusTotal behaviour summaries\",\n",
    "        \"domain\": \"mitre-enterprise\",\n",
    "        \"version\": \"4.3\",\n",
    "        \"techniques\": techniques\n",
    "    }\n",
    "\n",
    "print(\"âœ… ATT&CK layer builder ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e3243",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 10 â€” Collection Loop (rate-limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe51fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Collector ready\n"
     ]
    }
   ],
   "source": [
    "def collect_samples(sample_df: pd.DataFrame,\n",
    "                    delay_between_requests: float = DELAY_BETWEEN_REQUESTS,\n",
    "                    save_every: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each hash:\n",
    "      1) GET file report (save raw)\n",
    "      2) sleep\n",
    "      3) GET behaviour summary (save raw)\n",
    "      4) parse indicators â†’ row\n",
    "      5) rate-limit sleeps between samples\n",
    "    Saves partial results every `save_every` rows.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    tech_counter = Counter()\n",
    "\n",
    "    for idx, row in enumerate(tqdm(sample_df.to_dict(orient=\"records\"), desc=\"Collecting samples\"), start=1):\n",
    "        h = row['hash']\n",
    "        family = row.get('malware_family', 'Unknown')\n",
    "        source = row.get('source', 'Unknown')\n",
    "\n",
    "        # Initialize result row with defaults\n",
    "        result = {\n",
    "            'hash': h,\n",
    "            'family': family,\n",
    "            'source': source,\n",
    "            'status': 'failed',\n",
    "            'detection_ratio': None,\n",
    "            'first_seen': None,\n",
    "            'last_seen': None,\n",
    "            'processes_count': 0,\n",
    "            'files_written_count': 0,\n",
    "            'files_deleted_count': 0,\n",
    "            'registry_keys_set_count': 0,\n",
    "            'dns_lookups_count': 0,\n",
    "            'ip_connections_count': 0,\n",
    "            'http_requests_count': 0,\n",
    "            'mutexes_count': 0,\n",
    "            'mitre_techniques_count': 0,\n",
    "            'mitre_techniques': '',\n",
    "            'collected_date': datetime.now(timezone.utc).isoformat(),\n",
    "        }\n",
    "\n",
    "        # ---- 1) File report\n",
    "        fr = get_file_report(h)\n",
    "        with open(os.path.join(RAW_DIR, f\"{h}_file.json\"), \"w\") as fh:\n",
    "            json.dump(fr, fh, indent=2, default=str)\n",
    "\n",
    "        if not fr.get('success'):\n",
    "            # Record failure and continue (still sleep to respect rate limits)\n",
    "            result['error'] = fr.get('error', 'file_report_failed')\n",
    "            results.append(result)\n",
    "            time.sleep(delay_between_requests)\n",
    "            continue\n",
    "\n",
    "        # Parse some fields (safe access)\n",
    "        try:\n",
    "            attrs = fr['json'].get('data', {}).get('attributes', {}) if fr.get('json') else {}\n",
    "            stats = attrs.get('last_analysis_stats', {})\n",
    "            total = sum(stats.values()) if isinstance(stats, dict) else 0\n",
    "            malicious = stats.get('malicious', 0) if isinstance(stats, dict) else 0\n",
    "            result['detection_ratio'] = f\"{malicious}/{total}\" if total else None\n",
    "            result['first_seen'] = attrs.get('first_submission_date') or None\n",
    "            result['last_seen'] = attrs.get('last_analysis_date') or None\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Error extracting stats for {h}: {e}\")\n",
    "\n",
    "        # ---- rate-limit sleep between major API calls\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "        # ---- 2) Behaviour summary\n",
    "        br = get_behavior_report(h)\n",
    "        with open(os.path.join(RAW_DIR, f\"{h}_behavior.json\"), \"w\") as fh:\n",
    "            json.dump(br, fh, indent=2, default=str)\n",
    "\n",
    "        # Parse behaviour if success; else keep empties\n",
    "        if not br.get('success'):\n",
    "            indicators = {\n",
    "                'processes_created': [], 'files_written': [], 'files_deleted': [],\n",
    "                'registry_keys_set': [], 'registry_keys_deleted': [], 'dns_lookups': [],\n",
    "                'ip_traffic': [], 'http_conversations': [], 'command_executions': [],\n",
    "                'mutexes_created': [], 'services_created': [], 'mitre_techniques': []\n",
    "            }\n",
    "        else:\n",
    "            indicators = extract_behavioral_indicators(br)\n",
    "\n",
    "        # Update counts and technique list\n",
    "        result.update({\n",
    "            'processes_count': len(indicators['processes_created']),\n",
    "            'files_written_count': len(indicators['files_written']),\n",
    "            'files_deleted_count': len(indicators['files_deleted']),\n",
    "            'registry_keys_set_count': len(indicators['registry_keys_set']),\n",
    "            'dns_lookups_count': len(indicators['dns_lookups']),\n",
    "            'ip_connections_count': len(indicators['ip_traffic']),\n",
    "            'http_requests_count': len(indicators['http_conversations']),\n",
    "            'mutexes_count': len(indicators['mutexes_created']),\n",
    "            'mitre_techniques_count': len(indicators['mitre_techniques']),\n",
    "            'mitre_techniques': \", \".join([t.get('id','') for t in indicators['mitre_techniques'] if isinstance(t, dict)])\n",
    "        })\n",
    "\n",
    "        # Count techniques globally (for Navigator layer)\n",
    "        for t in indicators['mitre_techniques']:\n",
    "            if isinstance(t, dict) and t.get('id'):\n",
    "                tech_counter[t['id']] += 1\n",
    "\n",
    "        # Mark row success and append\n",
    "        result['status'] = 'success'\n",
    "        results.append(result)\n",
    "\n",
    "        # Periodic save of partial progress\n",
    "        if idx % save_every == 0 or idx == len(sample_df):\n",
    "            df_partial = pd.DataFrame(results)\n",
    "            df_partial.to_csv(os.path.join(OUTPUT_DIR, \"analysis_results_partial.csv\"), index=False)\n",
    "            with open(os.path.join(OUTPUT_DIR, \"analysis_results_partial.json\"), \"w\") as fh:\n",
    "                json.dump(results, fh, indent=2)\n",
    "            logging.info(f\"Saved partial results at {idx}/{len(sample_df)}\")\n",
    "\n",
    "        # ---- rate-limit sleep between samples\n",
    "        if idx < len(sample_df):\n",
    "            time.sleep(delay_between_requests)\n",
    "\n",
    "    # Save final, full outputs\n",
    "    df_final = pd.DataFrame(results)\n",
    "    df_final.to_csv(os.path.join(OUTPUT_DIR, \"analysis_results.csv\"), index=False)\n",
    "    df_final.to_json(os.path.join(OUTPUT_DIR, \"analysis_results.json\"), orient=\"records\", indent=2)\n",
    "\n",
    "    # Build ATT&CK Navigator layer\n",
    "    layer_json = build_attck_layer(tech_counter)\n",
    "    with open(os.path.join(OUTPUT_DIR, \"attck_navigator_layer.json\"), \"w\") as fh:\n",
    "        json.dump(layer_json, fh, indent=2)\n",
    "\n",
    "    logging.info(\"Collection complete. Outputs saved to ./output\")\n",
    "    return results\n",
    "\n",
    "print(\"âœ… Collector ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57ba14",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 11 â€” Main Driver (select + estimate + run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "447ffbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Main ready\n"
     ]
    }
   ],
   "source": [
    "def main(input_csv: str = INPUT_CSV,\n",
    "         min_samples: int = TARGET_MIN_SAMPLES,\n",
    "         max_samples: int = TARGET_MAX_SAMPLES,\n",
    "         min_families: int = MIN_FAMILIES):\n",
    "    # 1) Read input hashes\n",
    "    logging.info(\"Reading input CSV...\")\n",
    "    df = read_hash_csv(input_csv)\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"No hashes found in input CSV. Exiting.\")\n",
    "\n",
    "    # 2) Enforce assignment minimums\n",
    "    assert_dataset_meets_requirements(df, min_samples=min_samples, min_families=min_families)\n",
    "\n",
    "    # 3) Build a balanced selection\n",
    "    samples = select_sample_hashes(df, min_samples=min_samples, max_samples=max_samples, min_families=min_families)\n",
    "    logging.info(f\"Beginning collection of {len(samples)} samples (delay {DELAY_BETWEEN_REQUESTS}s between major API calls)\")\n",
    "\n",
    "    # 4) Print planned sleep time (excludes network/IO/retries)\n",
    "    n = len(samples)\n",
    "    planned_sleeps = (2 * n) - 1\n",
    "    planned_wait_seconds = planned_sleeps * DELAY_BETWEEN_REQUESTS\n",
    "    print(f\"Planned sleeps: {planned_sleeps} Ã— {DELAY_BETWEEN_REQUESTS:.1f}s \"\n",
    "          f\"= {int(planned_wait_seconds)}s ({planned_wait_seconds/60:.1f} minutes) \"\n",
    "          \"(excludes network/retries/IO).\")\n",
    "\n",
    "    # 5) Confirm run (set CONFIRM=yes to skip)\n",
    "    if os.getenv(\"CONFIRM\", \"no\").lower() != \"yes\":\n",
    "        print(f\"About to query VirusTotal for {len(samples)} hashes. This respects rate-limits \"\n",
    "              f\"({DELAY_BETWEEN_REQUESTS}s per major call).\")\n",
    "        proceed = input(\"Proceed? (yes/no): \").strip().lower()\n",
    "        if proceed != \"yes\":\n",
    "            logging.info(\"User aborted.\")\n",
    "            return\n",
    "\n",
    "    # 6) Collect + save outputs\n",
    "    results = collect_samples(samples)\n",
    "\n",
    "    # 7) Short summary\n",
    "    df_res = pd.DataFrame(results)\n",
    "    success_count = (df_res['status'] == 'success').sum()\n",
    "    logging.info(f\"Finished. Success: {success_count}/{len(df_res)}\")\n",
    "    logging.info(\"Open output/attck_navigator_layer.json in ATT&CK Navigator to visualize technique coverage.\")\n",
    "\n",
    "print(\"âœ… Main ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0d194",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 12 â€” Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e6fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 23:17:51,769 - INFO - Reading input CSV...\n",
      "2025-11-03 23:17:51,792 - INFO - Available families in input: 23\n",
      "2025-11-03 23:17:51,793 - INFO - Target sample count: 100\n",
      "2025-11-03 23:17:51,808 - INFO - Selected 100 samples from 23 families\n",
      "2025-11-03 23:17:51,809 - INFO - Beginning collection of 100 samples (delay 16.0s between major API calls)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned sleeps: 199 Ã— 16.0s = 3184s (53.1 minutes) (excludes network/retries/IO).\n",
      "About to query VirusTotal for 100 hashes. This respects rate-limits (16.0s per major call).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting samples:   9%|â–‰         | 9/100 [04:54<49:38, 32.73s/it]2025-11-03 23:23:08,364 - INFO - Saved partial results at 10/100\n",
      "Collecting samples:  19%|â–ˆâ–‰        | 19/100 [10:23<44:30, 32.97s/it]2025-11-03 23:28:36,974 - INFO - Saved partial results at 20/100\n",
      "Collecting samples:  29%|â–ˆâ–ˆâ–‰       | 29/100 [16:04<41:13, 34.83s/it]2025-11-03 23:34:17,631 - INFO - Saved partial results at 30/100\n",
      "Collecting samples:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [21:32<33:23, 32.84s/it]2025-11-03 23:39:45,719 - INFO - Saved partial results at 40/100\n",
      "Collecting samples:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [27:01<28:14, 33.23s/it]2025-11-03 23:45:15,473 - INFO - Saved partial results at 50/100\n",
      "Collecting samples:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [32:28<22:15, 32.58s/it]2025-11-03 23:50:41,543 - INFO - Saved partial results at 60/100\n",
      "Collecting samples:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [40:02<19:13, 37.20s/it]2025-11-03 23:58:16,674 - INFO - Saved partial results at 70/100\n",
      "Collecting samples:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [45:34<11:37, 33.21s/it]2025-11-04 00:03:48,147 - INFO - Saved partial results at 80/100\n",
      "Collecting samples:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [51:03<06:03, 33.07s/it]2025-11-04 00:09:17,276 - INFO - Saved partial results at 90/100\n",
      "Collecting samples:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [56:30<00:32, 32.70s/it]2025-11-04 00:14:43,574 - INFO - Saved partial results at 100/100\n",
      "Collecting samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [56:46<00:00, 34.07s/it]\n",
      "2025-11-04 00:14:43,580 - INFO - Collection complete. Outputs saved to ./output\n",
      "2025-11-04 00:14:43,582 - INFO - Finished. Success: 100/100\n",
      "2025-11-04 00:14:43,582 - INFO - Open output/attck_navigator_layer.json in ATT&CK Navigator to visualize technique coverage.\n"
     ]
    }
   ],
   "source": [
    "# Run the main flow. You can override INPUT_CSV or min/max here if needed.\n",
    "main(\n",
    "    input_csv=INPUT_CSV,\n",
    "    min_samples=TARGET_MIN_SAMPLES,\n",
    "    max_samples=TARGET_MAX_SAMPLES,\n",
    "    min_families=MIN_FAMILIES\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
