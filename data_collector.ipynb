{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d08528",
   "metadata": {},
   "source": [
    "# Malware Analysis Data Collector\n",
    "\n",
    "This notebook implements a comprehensive malware analysis pipeline using the VirusTotal API. The goal is to:\n",
    "1. Collect metadata and behavioral data for different malware families\n",
    "2. Extract key behavioral indicators\n",
    "3. Map behaviors to MITRE ATT&CK techniques\n",
    "4. Generate detailed analysis reports\n",
    "\n",
    "The analysis focuses on several malware families including:\n",
    "- Emotet/Heodo\n",
    "- Dridex\n",
    "- AgentTesla\n",
    "- Tinba\n",
    "- TrickBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3370b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8675b8",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "First, let's install the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries for data processing and file operations\n",
    "import requests  # For making HTTP requests to VirusTotal API\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import csv  # For reading/writing CSV files\n",
    "import json  # For JSON data handling\n",
    "import time  # For implementing delays between API requests\n",
    "from typing import List, Dict  # Type hints for better code readability\n",
    "from datetime import datetime  # For timestamps\n",
    "import os  # For file/directory operations\n",
    "\n",
    "print(\"Required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70719599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION =====\n",
    "\n",
    "# VirusTotal API configuration\n",
    "API_KEY = '2117ff9ed05bbfde342deec3c7e417fa98cd4068adb477f43ac3c1d58e29431a'  # Your VirusTotal API key\n",
    "\n",
    "# Rate limiting configuration \n",
    "# Free tier allows 4 requests/minute, so we need 15 seconds between requests\n",
    "# Using 16 seconds to be safe\n",
    "DELAY_BETWEEN_REQUESTS = 16  # seconds\n",
    "\n",
    "# Set up request headers with API key\n",
    "headers = {\"x-apikey\": API_KEY}\n",
    "\n",
    "print(\"Configuration completed:\")\n",
    "print(f\"- API Key configured: {'‚úì' if API_KEY else '‚úó'}\")\n",
    "print(f\"- Rate limit delay: {DELAY_BETWEEN_REQUESTS} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== HELPER FUNCTIONS =====\n",
    "\n",
    "def read_hash_file(file_name: str, hash_col: str = \"hash\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Read malware hash values and related information from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the CSV file containing hash data\n",
    "        hash_col (str): Name of the column containing hash values (default: \"hash\")\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of dictionaries containing hash info and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Reading hash data from {file_name}...\")\n",
    "    \n",
    "    # List to store hash info\n",
    "    hash_data = []\n",
    "    try:\n",
    "        with open(file_name, newline='', encoding='utf-8') as fh:\n",
    "            reader = csv.DictReader(fh)  # Open CSV as dictionary reader\n",
    "            \n",
    "            # Check if headers exist\n",
    "            if reader.fieldnames is None:\n",
    "                raise ValueError(\"CSV file appears to have no header row.\")\n",
    "            \n",
    "            # Ensure hash column exists\n",
    "            if hash_col not in reader.fieldnames:\n",
    "                raise ValueError(f\"Column '{hash_col}' not found in CSV header: {reader.fieldnames}\")\n",
    "            \n",
    "            # Iterate each row and extract hash and related info\n",
    "            print(\"Processing rows...\")\n",
    "            for row in reader:\n",
    "                raw_hash = row.get(hash_col, \"\").strip().lower()\n",
    "                # Skip empty or example hashes\n",
    "                if raw_hash and raw_hash != \"example_hash_here\":\n",
    "                    hash_data.append({\n",
    "                        'hash': raw_hash,\n",
    "                        'family': row.get('malware_family', 'Unknown'),\n",
    "                        'source': row.get('source', 'Unknown')\n",
    "                    })\n",
    "        \n",
    "        # Print number of hashes loaded\n",
    "        print(f\"‚úì Successfully loaded {len(hash_data)} hashes\")\n",
    "        return hash_data\n",
    "    \n",
    "    # Handle file not found error\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó Error: File '{file_name}' not found!\")\n",
    "        return []\n",
    "    # Handle other exceptions\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error reading file: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff610d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_report(hash_val: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Retrieve file report from VirusTotal API for a given hash.\n",
    "    \n",
    "    Args:\n",
    "        hash_val (str): The hash value to lookup\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Response data with success status and report/error info\n",
    "    \"\"\"\n",
    "    url = f\"https://www.virustotal.com/api/v3/files/{hash_val}\"\n",
    "    try:\n",
    "        print(f\"  ‚Üí API Request: GET {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Successful response\n",
    "        if response.status_code == 200:\n",
    "            print(\"  ‚úì Request successful\")\n",
    "            return {'success': True, 'data': response.json()}\n",
    "        # Hash not found on VirusTotal\n",
    "        elif response.status_code == 404:\n",
    "            print(\"  ‚úó Hash not found on VirusTotal\")\n",
    "            return {'success': False, 'error': 'Hash not found on VirusTotal'}\n",
    "        # Other errors\n",
    "        else:\n",
    "            print(f\"  ‚úó Request failed: HTTP {response.status_code}\")\n",
    "            return {'success': False, 'error': f'Error {response.status_code}'}\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Request error: {str(e)}\")\n",
    "        return {'success': False, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_behavior_report(hash_val: str) -> Dict:\n",
    "    url = f\"https://www.virustotal.com/api/v3/files/{hash_val}/behaviour_summary\"\n",
    "    try:\n",
    "        print(f\"  ‚Üí API Request: GET {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            print(\"  ‚úì Request successful\")\n",
    "            return {'success': True, 'data': response.json()}\n",
    "        elif response.status_code == 404:\n",
    "            print(\"  ‚úó No behavior data available\")\n",
    "            return {'success': False, 'error': 'No behavior data available'}\n",
    "        else:\n",
    "            print(f\"  ‚úó Request failed: HTTP {response.status_code}\")\n",
    "            return {'success': False, 'error': f'Error {response.status_code}'}\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Request error: {str(e)}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "def extract_behavioral_indicators(behavior_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract behavioral indicators from VirusTotal behavior data.\n",
    "    \n",
    "    Args:\n",
    "        behavior_data (Dict): Behavioral data from VirusTotal API\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Structured behavioral indicators\n",
    "    \"\"\"\n",
    "    print(\"  ‚Üí Extracting behavioral indicators...\")\n",
    "\n",
    "    # Initialize with empty structure FIRST\n",
    "    indicators = {\n",
    "        'processes_created': [],\n",
    "        'files_written': [],\n",
    "        'files_deleted': [],\n",
    "        'registry_keys_set': [],\n",
    "        'registry_keys_deleted': [],\n",
    "        'dns_lookups': [],\n",
    "        'ip_traffic': [],\n",
    "        'http_conversations': [],\n",
    "        'command_executions': [],\n",
    "        'mutexes_created': [],\n",
    "        'services_created': [],\n",
    "        'mitre_techniques': []\n",
    "    }\n",
    "\n",
    "    # Check for invalid input - return empty indicators if invalid\n",
    "    if behavior_data is None or not isinstance(behavior_data, dict):\n",
    "        print(\"  ‚úó behavior_data is None or not a dict\")\n",
    "        return indicators\n",
    "\n",
    "    if not behavior_data.get('success', False):\n",
    "        print(\"  ‚úó No successful behavioral data\")\n",
    "        return indicators\n",
    "\n",
    "    # Safe data access with proper error handling\n",
    "    try:\n",
    "        # Navigate the nested structure safely\n",
    "        data = behavior_data.get('data', {})\n",
    "        if isinstance(data, dict):\n",
    "            inner_data = data.get('data', {})\n",
    "            if isinstance(inner_data, dict):\n",
    "                attributes = inner_data.get('attributes', {})\n",
    "            else:\n",
    "                attributes = {}\n",
    "        else:\n",
    "            attributes = {}\n",
    "\n",
    "        # Extract each indicator type with safety checks\n",
    "        if 'processes_created' in attributes:\n",
    "            indicators['processes_created'] = attributes.get('processes_created', [])\n",
    "        \n",
    "        if 'files_written' in attributes:\n",
    "            indicators['files_written'] = attributes.get('files_written', [])\n",
    "        \n",
    "        if 'files_deleted' in attributes:\n",
    "            indicators['files_deleted'] = attributes.get('files_deleted', [])\n",
    "        \n",
    "        if 'registry_keys_set' in attributes:\n",
    "            indicators['registry_keys_set'] = attributes.get('registry_keys_set', [])\n",
    "        \n",
    "        if 'registry_keys_deleted' in attributes:\n",
    "            indicators['registry_keys_deleted'] = attributes.get('registry_keys_deleted', [])\n",
    "        \n",
    "        if 'dns_lookups' in attributes:\n",
    "            dns_data = attributes.get('dns_lookups', [])\n",
    "            indicators['dns_lookups'] = [\n",
    "                lookup.get('hostname', '') for lookup in dns_data \n",
    "                if isinstance(lookup, dict)\n",
    "            ]\n",
    "        \n",
    "        if 'ip_traffic' in attributes:\n",
    "            ip_data = attributes.get('ip_traffic', [])\n",
    "            indicators['ip_traffic'] = [\n",
    "                f\"{ip.get('destination_ip', '')}:{ip.get('destination_port', '')}\" \n",
    "                for ip in ip_data if isinstance(ip, dict)\n",
    "            ]\n",
    "        \n",
    "        if 'http_conversations' in attributes:\n",
    "            http_data = attributes.get('http_conversations', [])\n",
    "            indicators['http_conversations'] = [\n",
    "                conv.get('url', '') for conv in http_data \n",
    "                if isinstance(conv, dict)\n",
    "            ]\n",
    "        \n",
    "        if 'command_executions' in attributes:\n",
    "            indicators['command_executions'] = attributes.get('command_executions', [])\n",
    "        \n",
    "        if 'mutexes_created' in attributes:\n",
    "            indicators['mutexes_created'] = attributes.get('mutexes_created', [])\n",
    "        \n",
    "        if 'services_created' in attributes:\n",
    "            indicators['services_created'] = attributes.get('services_created', [])\n",
    "        \n",
    "        if 'mitre_attack_techniques' in attributes:\n",
    "            mitre_data = attributes.get('mitre_attack_techniques', [])\n",
    "            indicators['mitre_techniques'] = [\n",
    "                tech for tech in mitre_data if isinstance(tech, dict)\n",
    "            ]\n",
    "\n",
    "        print(\"  ‚úì Behavioral indicators extracted successfully\")\n",
    "        return indicators\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error extracting behavioral indicators: {str(e)}\")\n",
    "        return indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43526667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for all malware hash samples and save results to files\n",
    "def collect_sample_data(hash_data_list: List[Dict], output_dir: str = \"output\"):\n",
    "    # Create output directories if they do not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/raw_responses\", exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    total = len(hash_data_list)\n",
    "    \n",
    "    # Print start info and estimated total time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting data collection for {total} samples...\")\n",
    "    print(f\"Estimated time: {(total * DELAY_BETWEEN_REQUESTS) / 60:.1f} minutes\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Loop through hashes one by one\n",
    "    for idx, hash_info in enumerate(hash_data_list, 1):\n",
    "        hash_val = hash_info['hash']\n",
    "        family = hash_info['family']\n",
    "        \n",
    "        print(f\"[{idx}/{total}] Processing: {hash_val[:16]}... ({family})\")\n",
    "        \n",
    "        # Get file report from VirusTotal\n",
    "        print(f\"  ‚Üí Fetching file report...\")\n",
    "        file_report = get_file_report(hash_val)\n",
    "        \n",
    "        # If file report failed, record failure and continue to next item\n",
    "        if not file_report['success']:\n",
    "            print(f\"  ‚úó {file_report['error']}\")\n",
    "            results.append({\n",
    "                'hash': hash_val,\n",
    "                'family': family,\n",
    "                'status': 'failed',\n",
    "                'error': file_report['error']\n",
    "            })\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "            continue\n",
    "        \n",
    "        # Wait to respect rate limit\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "        \n",
    "        # Get behavioral report from VirusTotal\n",
    "        print(f\"  ‚Üí Fetching behavior report...\")\n",
    "        behavior_report = get_behavior_report(hash_val)\n",
    "        \n",
    "        # Save raw JSON responses to files\n",
    "        with open(f\"{output_dir}/raw_responses/{hash_val}_file.json\", 'w') as f:\n",
    "            json.dump(file_report, f, indent=2)\n",
    "        with open(f\"{output_dir}/raw_responses/{hash_val}_behavior.json\", 'w') as f:\n",
    "            json.dump(behavior_report, f, indent=2)\n",
    "        \n",
    "        # Extract behavioral indicators from sandbox data\n",
    "        indicators = extract_behavioral_indicators(behavior_report)\n",
    "        \n",
    "        # Prepare result dictionary to store counts and info\n",
    "        result = {\n",
    "            'hash': hash_val,\n",
    "            'family': family,\n",
    "            'source': hash_info['source'],\n",
    "            'status': 'success',\n",
    "            'detection_ratio': None,\n",
    "            'first_seen': None,\n",
    "            'last_seen': None,\n",
    "            'processes_count': len(indicators['processes_created']),\n",
    "            'files_written_count': len(indicators['files_written']),\n",
    "            'files_deleted_count': len(indicators['files_deleted']),\n",
    "            'registry_keys_set_count': len(indicators['registry_keys_set']),\n",
    "            'dns_lookups_count': len(indicators['dns_lookups']),\n",
    "            'ip_connections_count': len(indicators['ip_traffic']),\n",
    "            'http_requests_count': len(indicators['http_conversations']),\n",
    "            'mutexes_count': len(indicators['mutexes_created']),\n",
    "            'mitre_techniques_count': len(indicators['mitre_techniques']),\n",
    "            'mitre_techniques': ', '.join([t.get('id', '') for t in indicators['mitre_techniques']]),\n",
    "            'collected_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        # Get detection stats and timestamps from file report, if available\n",
    "        if file_report['success']:\n",
    "            attrs = file_report['data'].get('data', {}).get('attributes', {})\n",
    "            stats = attrs.get('last_analysis_stats', {})\n",
    "            result['detection_ratio'] = f\"{stats.get('malicious', 0)}/{sum(stats.values())}\"\n",
    "            result['first_seen'] = attrs.get('first_submission_date', 'N/A')\n",
    "            result['last_seen'] = attrs.get('last_analysis_date', 'N/A')\n",
    "        \n",
    "        # Add current result to results list\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  ‚úì Success! Detections: {result['detection_ratio']}, \"\n",
    "              f\"MITRE Techniques: {result['mitre_techniques_count']}\")\n",
    "        \n",
    "        # Save progress every 5 samples\n",
    "        if idx % 5 == 0:\n",
    "            df_temp = pd.DataFrame(results)\n",
    "            df_temp.to_csv(f\"{output_dir}/analysis_results_partial.csv\", index=False)\n",
    "            print(f\"\\n  üíæ Progress saved ({idx}/{total} completed)\\n\")\n",
    "        \n",
    "        # Wait before next request (except after last one)\n",
    "        if idx < total:\n",
    "            print(f\"  ‚è≥ Waiting {DELAY_BETWEEN_REQUESTS}s (rate limit)...\\n\")\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "    \n",
    "    # Save final results as CSV and JSON\n",
    "    df_final = pd.DataFrame(results)\n",
    "    df_final.to_csv(f\"{output_dir}/analysis_results.csv\", index=False)\n",
    "    df_final.to_json(f\"{output_dir}/analysis_results.json\", orient=\"records\", indent=2)\n",
    "    \n",
    "    # Summary output to console\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Data collection complete!\")\n",
    "    print(f\"  Total samples: {total}\")\n",
    "    print(f\"  Successful: {sum(1 for r in results if r['status'] == 'success')}\")\n",
    "    print(f\"  Failed: {sum(1 for r in results if r['status'] == 'failed')}\")\n",
    "    print(f\"\\nResults saved to:\")\n",
    "    print(f\"  - {output_dir}/analysis_results.csv\")\n",
    "    print(f\"  - {output_dir}/analysis_results.json\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate a summary report based on collected results\n",
    "def generate_summary_report(results: List[Dict], output_dir: str = \"output\"):\n",
    "    # Convert results list to dataframe for analysis\n",
    "    df = pd.DataFrame(results)\n",
    "    # Filter only successful samples\n",
    "    successful = df[df['status'] == 'success']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COLLECTION SUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Print basic summary counts\n",
    "    print(f\"\\nTotal samples processed: {len(results)}\")\n",
    "    print(f\"Successful collections: {len(successful)}\")\n",
    "    print(f\"Failed collections: {len(results) - len(successful)}\")\n",
    "    \n",
    "    if len(successful) > 0:\n",
    "        # Show count by malware family\n",
    "        print(f\"\\nSamples by Malware Family:\")\n",
    "        family_counts = successful['family'].value_counts()\n",
    "        for family, count in family_counts.items():\n",
    "            print(f\"  {family}: {count}\")\n",
    "        \n",
    "        # Show averages of behavioral indicators\n",
    "        print(f\"\\nBehavioral Indicators Summary:\")\n",
    "        print(f\"  Avg processes created: {successful['processes_count'].mean():.1f}\")\n",
    "        print(f\"  Avg files written: {successful['files_written_count'].mean():.1f}\")\n",
    "        print(f\"  Avg registry modifications: {successful['registry_keys_set_count'].mean():.1f}\")\n",
    "        print(f\"  Avg DNS lookups: {successful['dns_lookups_count'].mean():.1f}\")\n",
    "        print(f\"  Avg network connections: {successful['ip_connections_count'].mean():.1f}\")\n",
    "        \n",
    "        # MITRE ATT&CK technique coverage info\n",
    "        print(f\"\\nMITRE ATT&CK Coverage:\")\n",
    "        print(f\"  Samples with MITRE techniques: {(successful['mitre_techniques_count'] > 0).sum()}\")\n",
    "        print(f\"  Total unique techniques identified: {successful['mitre_techniques'].nunique()}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23673a70",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response == \u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Collect data and save to files\u001b[39;00m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müöÄ Starting analysis pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     results = \u001b[43mcollect_sample_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhash_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Print summary report\u001b[39;00m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Generating analysis report...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mcollect_sample_data\u001b[39m\u001b[34m(hash_data_list, output_dir)\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Wait to respect rate limit\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDELAY_BETWEEN_REQUESTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Get behavioral report from VirusTotal\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚Üí Fetching behavior report...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===== MAIN EXECUTION =====\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VirusTotal Malware Analysis Pipeline\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Define paths\n",
    "INPUT_DIR = \"./hash_data\"\n",
    "OUTPUT_DIR = \"./output\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"‚úì Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# First, we'll convert our JSON data to a CSV format\n",
    "def convert_json_to_csv():\n",
    "    \"\"\"Convert the JSON malware data files to a single CSV for processing\"\"\"\n",
    "    print(\"\\nüîÑ Converting JSON data to CSV format...\")\n",
    "    \n",
    "    all_samples = []\n",
    "    json_files = ['AgentTesla.json', 'Dridex.json', 'all_7_families.json']\n",
    "    \n",
    "    for file in json_files:\n",
    "        try:\n",
    "            with open(os.path.join(INPUT_DIR, file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, dict) and 'data' in data:\n",
    "                    samples = data['data']\n",
    "                else:\n",
    "                    samples = data\n",
    "                family = file.replace('.json', '')\n",
    "                for sample in samples:\n",
    "                    all_samples.append({\n",
    "                        'hash': sample['sha256_hash'],\n",
    "                        'malware_family': sample['signature'] or family,\n",
    "                        'source': sample['reporter']\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error processing {file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_csv = os.path.join(OUTPUT_DIR, 'hash_signature_output.csv')\n",
    "    df = pd.DataFrame(all_samples)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"‚úì Created CSV file with {len(df)} samples: {output_csv}\")\n",
    "    return output_csv\n",
    "\n",
    "# Convert JSON to CSV\n",
    "input_file = convert_json_to_csv()\n",
    "\n",
    "# Read hashes to process\n",
    "print(\"\\nüìù Reading hash data...\")\n",
    "hash_data = read_hash_file(input_file)\n",
    "\n",
    "# Exit if no hashes found\n",
    "if not hash_data:\n",
    "    print(\"‚úó No hashes found. Please check your input file.\")\n",
    "    exit(1)\n",
    "\n",
    "# Show number of hashes and preview first 3\n",
    "print(f\"\\nüìä Found {len(hash_data)} samples to analyze\")\n",
    "print(\"\\nPreview of samples:\")\n",
    "for i, h in enumerate(hash_data[:3], 1):\n",
    "    print(f\"  {i}. {h['hash'][:16]}... ({h['family']})\")\n",
    "if len(hash_data) > 3:\n",
    "    print(f\"  ... and {len(hash_data) - 3} more\")\n",
    "\n",
    "# Ask user to confirm before starting data collection\n",
    "response = input(\"\\n‚ö†Ô∏è Proceed with data collection? (yes/no): \").strip().lower()\n",
    "\n",
    "if response == 'yes':\n",
    "    # Collect data and save to files\n",
    "    print(\"\\nüöÄ Starting analysis pipeline...\")\n",
    "    results = collect_sample_data(hash_data, OUTPUT_DIR)\n",
    "    \n",
    "    # Print summary report\n",
    "    print(\"\\nüìä Generating analysis report...\")\n",
    "    generate_summary_report(results, OUTPUT_DIR)\n",
    "    \n",
    "    print(\"\\n‚ú® Analysis pipeline completed successfully!\")\n",
    "    print(\"üìÅ Check the output folder for detailed results.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Analysis cancelled by user.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
